{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC\n",
    "\n",
    "- [Libraries](#libraries)\n",
    "- [Scraping STANOX codes](#scraping-stanox-codes)\n",
    "   * [Scrape Railway Codes pages](#scrape-railway-codes-pages)\n",
    "   * [Clean Stanox Mapping Dataframe](#clean-stanox-mapping-dataframe)\n",
    "   * [Save Stanox Mapping Dictionary to .CSV](#save-stanox-mapping-dictionary-to-csv)\n",
    "   * [Package code into functions](#package-code-into-functions)\n",
    "- [Scraping TOC codes](#scraping-toc-codes)\n",
    "   * [Scrape Railway Codes pages](#scrape-railway-codes-pages-1)\n",
    "   * [Clean TOC Codes Mapping Dataframe](#clean-toc-codes-mapping-dataframe)\n",
    "   * [Save TOC Codes Mapping to .CSV](#save-toc-codes-mapping-to-csv)\n",
    "   * [Package code into functions](#package-code-into-functions-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"libraries\"></a>\n",
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"scraping-stanox-codes\"></a>\n",
    "# Scraping STANOX codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"scrape-railway-codes-pages\"></a>\n",
    "## Scrape Railway Codes pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of dfs to fill with each letter dataframe\n",
    "list_dfs = []\n",
    "for letter in list(string.ascii_lowercase):\n",
    "    url = f\"http://www.railwaycodes.org.uk/crs/crs{letter}.shtm\"\n",
    "    response = requests.get(url=url, headers={\"Accept-Language\":\"en-US\"})\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        # in each page select the table identified by the id \"tablesort\"\n",
    "        table = soup.find('table', {'id': 'tablesort'})\n",
    "        # initialize empty data and headers list\n",
    "        data = []\n",
    "        headers = []\n",
    "        # extract header row\n",
    "        header_row = table.find('tr')\n",
    "        for th in header_row.find_all('th'):\n",
    "            headers.append(th.text.strip())\n",
    "        # extract rows\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            row_data = [td.text.strip() for td in row.find_all('td')]\n",
    "            data.append(row_data)\n",
    "        # create df\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        list_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Letter {url} could not be scraped.\")\n",
    "\n",
    "stanox_mapping_df = pd.concat(list_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"clean-stanox-mapping-dataframe\"></a>\n",
    "## Clean Stanox Mapping Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values: Dropping 0 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/hplzffs15b10rh0yk5wmdmkw0000gn/T/ipykernel_12132/2815815964.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  print(f\"Empty Values: Dropping {stanox_mapping_df[empty_mask].shape[0]} rows\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Values: Dropping 0 rows\n"
     ]
    }
   ],
   "source": [
    "#drop rows where Stanox codes are null\n",
    "null_mask = stanox_mapping_df['STANOX'].isnull()\n",
    "stanox_mapping_df = stanox_mapping_df[~null_mask]\n",
    "print(f\"Null Values: Dropping {stanox_mapping_df[null_mask].shape[0]} rows\")\n",
    "#drop rows where Stanox codes are empty\n",
    "empty_mask = stanox_mapping_df['STANOX'] == \"\"\n",
    "stanox_mapping_df = stanox_mapping_df[~empty_mask]\n",
    "print(f\"Empty Values: Dropping {stanox_mapping_df[empty_mask].shape[0]} rows\")\n",
    "#drop redundant cols\n",
    "stanox_mapping_df = stanox_mapping_df[['Location', 'STANOX']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"save-stanox-mapping-dictionary-to-csv\"></a>\n",
    "## Save Stanox Mapping Dictionary to .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanox_mapping_df.to_csv(\"../raw_data/stanox_locations_mapping.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"package-code-into-functions\"></a>\n",
    "## Package code into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_stanox_codes():\n",
    "    \"\"\"\n",
    "    Scrapes CRS, NLC, TIPLOC and STANOX Codes from http://www.railwaycodes.org.uk/.\n",
    "    Returns:\n",
    "        dataframe : dataframe containing the output scraped from all the letters.\n",
    "    \"\"\"\n",
    "    # create a list of all letters\n",
    "    letters = list(string.ascii_lowercase)\n",
    "    # initialize list of dfs to fill with each letter dataframe\n",
    "    list_dfs = []\n",
    "    for letter in letters:\n",
    "        # for each letter build a custom URL\n",
    "        url = f\"http://www.railwaycodes.org.uk/crs/crs{letter}.shtm\"\n",
    "        # send request\n",
    "        response = requests.get(url=url, headers={\"Accept-Language\":\"en-US\"})\n",
    "        # if the request is successful\n",
    "        if response.status_code == 200:\n",
    "            # parse html page\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            # select the table identified by the id \"tablesort\" from the soup\n",
    "            table = soup.find('table', {'id': 'tablesort'})\n",
    "            # initialize empty data and headers list\n",
    "            data = []\n",
    "            headers = []\n",
    "            # extract header row\n",
    "            header_row = table.find('tr')\n",
    "            for th in header_row.find_all('th'):\n",
    "                headers.append(th.text.strip())\n",
    "            # extract rows\n",
    "            for row in table.find_all('tr')[1:]:\n",
    "                row_data = [td.text.strip() for td in row.find_all('td')[:6]]\n",
    "                data.append(row_data)\n",
    "            # create df\n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            # append df to list of dfs\n",
    "            list_dfs.append(df)\n",
    "        else:\n",
    "            # print the status code if the request failed\n",
    "            print(f\"Letter {url} could not be scraped. Status Code: {response.status_code}\")\n",
    "    # return a dataframe with all the information scraped for each page\n",
    "    return pd.concat(list_dfs)\n",
    "\n",
    "def clean_stanox_mapping_df(df):\n",
    "    \"\"\"\n",
    "    Cleans a dataframe obtained from scraping stanox codes.\n",
    "    Args:\n",
    "        df (dataframe) = DataFrame obtained by scraping stanox codes.\n",
    "\n",
    "    Returns:\n",
    "        dataframe : cleaned dataframe containing only the Location and STANOX columns.\n",
    "    \"\"\"\n",
    "    #drop rows where Stanox codes are null\n",
    "    null_mask = (df['STANOX'].isnull())\n",
    "    df = df[~null_mask]\n",
    "    #drop rows where Stanox codes are empty\n",
    "    empty_mask = (df['STANOX'] == \"\")\n",
    "    df = df[~empty_mask]\n",
    "    # drop rows where Stanox codes are \"-\"\"\n",
    "    invalid_mask = (df['STANOX'] == \"-\")\n",
    "    df = df[~invalid_mask]\n",
    "    #drop redundant cols\n",
    "    df = df[['Location', 'STANOX']]\n",
    "    return df\n",
    "\n",
    "def create_stanox_location_mapping(clean = True, to_csv = True):\n",
    "    \"\"\"\n",
    "    Creates a dataframe mapping STANOX codes and locations name.\n",
    "    Args:\n",
    "        clean (bool) = If true, rows with empty and null STANOX codes are dropped.\n",
    "                        Only STANOX and Location columns are kept.\n",
    "        to_csv (bool) = If true, the dataframe is exported in CSV to the ../raw_data folder.\n",
    "                        If folder does not exists, it is created.\n",
    "    Returns:\n",
    "        dataframe : cleaned dataframe containing only the Location and STANOX columns.\n",
    "    \"\"\"\n",
    "    # create a dataframe with all the data scraped from the http://www.railwaycodes.org.uk/\n",
    "    df = scrape_stanox_codes()\n",
    "    # if param clean is true, it removes empty, invalid and null STANOX codes. Keep only Location and Stanox Code column.\n",
    "    if clean == True:\n",
    "        df = clean_stanox_mapping_df(df)\n",
    "    if to_csv == True:\n",
    "    # if param csv is true, dataframe is also saved as a .csv in the ../raw_data folder.\n",
    "        folder_path = \"../raw_data\"\n",
    "        file_name = \"stanox_locations_mapping.csv\"\n",
    "        # if raw_data folder does not exists, create it.\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        df.to_csv(os.path.join(folder_path, file_name))\n",
    "        print(f\"{file_name} saved in {folder_path}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"scraping-toc-codes\"></a>\n",
    "# Scraping TOC codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"scrape-railway-codes-pages-1\"></a>\n",
    "## Scrape Railway Codes pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.railwaycodes.org.uk/operators/toccodes.shtm\"\n",
    "response = requests.get(url=url, headers={\"Accept-Language\":\"en-US\"})\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # in each page select the table identified by the id \"tablesort\"\n",
    "    table = soup.find('table', {'id': 'tablesort'})\n",
    "    # initialize empty data and headers list\n",
    "    data = []\n",
    "    headers = []\n",
    "    # extract header row\n",
    "    header_row = table.find('tr')\n",
    "    for th in header_row.find_all('th'):\n",
    "        headers.append(th.text.strip())\n",
    "    # extract rows\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        row_data = [td.text.strip() for td in row.find_all('td')]\n",
    "        data.append(row_data)\n",
    "    # create df\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "else:\n",
    "    print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"clean-toc-codes-mapping-dataframe\"></a>\n",
    "## Clean TOC Codes Mapping Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AM', 'AN', 'AR', 'AW', 'ATW', 'CA', 'CC', 'CH', 'CS', 'CT', 'CX',\n",
       "       'DC', 'EC', 'EM', 'EP', 'ES', 'EU', 'FC', 'GC', 'GE', 'GL', 'GM',\n",
       "       'GN', 'GR', 'GW', 'GWR', 'GX', 'HB', 'HC', 'HT', 'HX', 'IL', 'LD',\n",
       "       'LE', 'LF', 'LG', 'LM', 'LN', 'LO', 'LR', 'LS', 'LT', 'ME', 'ML',\n",
       "       'MV', 'NL', 'NR', 'NS', 'NT', 'NW', 'NY', 'QC', 'RE', 'RT', 'SC',\n",
       "       'SE', 'SJ', 'SN', 'SO', 'SP', 'SR', 'SS', 'SW', 'SX', 'TL', 'TP',\n",
       "       'TS', 'TT', 'TW', 'VL', 'VT', 'WB', 'WC', 'WE', 'WM', 'WN', 'WR',\n",
       "       'WS', 'WW', 'XC', 'XM', 'XP', 'XR', 'XS', 'XX', 'YG', 'ZZ', '#|'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only Code and Train Operator columns\n",
    "df = df[['Code','Train operator']]\n",
    "df['Code'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"save-toc-codes-to-csv\"></a>\n",
    "## Save TOC Codes Mapping to .CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../raw_data/toc_operators_mapping.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC --><a name=\"package-code-into-functions-1\"></a>\n",
    "## Package code into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toc_operator_mapping(clean = True, to_csv = True):\n",
    "    \"\"\"\n",
    "    Creates a dataframe mapping STANOX codes and locations name.\n",
    "    Args:\n",
    "        clean (bool) = If true, only Code and Train operators columns are kept.\n",
    "        to_csv (bool) = If true, the dataframe is exported in CSV to the ../raw_data folder.\n",
    "                        If folder does not exists, it is created.\n",
    "    Returns:\n",
    "        dataframe : cleaned dataframe containing only the Code and Train operators columns.\n",
    "    \"\"\"\n",
    "    url = \"http://www.railwaycodes.org.uk/operators/toccodes.shtm\"\n",
    "    response = requests.get(url=url, headers={\"Accept-Language\":\"en-US\"})\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        # in each page select the table identified by the id \"tablesort\"\n",
    "        table = soup.find('table', {'id': 'tablesort'})\n",
    "        # initialize empty data and headers list\n",
    "        data = []\n",
    "        headers = []\n",
    "        # extract header row\n",
    "        header_row = table.find('tr')\n",
    "        for th in header_row.find_all('th'):\n",
    "            headers.append(th.text.strip())\n",
    "        # extract rows\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            row_data = [td.text.strip() for td in row.find_all('td')]\n",
    "            data.append(row_data)\n",
    "        # create df\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "    if clean == True:\n",
    "        df = df[['Code','Train operator']]\n",
    "    if to_csv:\n",
    "    # if param csv is true, dataframe is also saved as a .csv in the ../raw_data folder.\n",
    "        folder_path = \"../raw_data\"\n",
    "        file_name = \"toc_operators_mapping.csv\"\n",
    "        # if raw_data folder does not exists, create it.\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        df.to_csv(os.path.join(folder_path, file_name))\n",
    "        print(f\"{file_name} saved in {folder_path}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
